<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Multi-stage yield prediction</title>
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="css/proj-styles.css">
    <link rel="stylesheet" href="css/bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="css/isotope.css" media="screen" />
    <link rel="stylesheet" href="js/fancybox/jquery.fancybox.css" type="text/css" media="screen" />
    <link rel="stylesheet" type="text/css" href="css/da-slider.css" />
    <link rel="stylesheet" href="css/styles.css" />
    <!-- Font Awesome -->
    <link href="css/font-awesome.min.css" rel="stylesheet">
</head>
<body style="display: flex; flex-direction: column; height: 100vh; margin: 0;">
    <!-- Title Section -->
    <header style="display: flex; justify-content: center; align-items: center; flex: 0 0 15%; width: 100%; margin: 0;">
        <h1 style="font-size: 3vw; text-align: center; margin: 0;">
            Advancing harvest table beet root yield estimation via unmanned aerial systems (UAS) multi-modal sensing
        </h1>
    </header>

    <!-- Main Content Section -->
    <main style="display: flex; flex: 1 0 85%; width: 100%; margin: 0;">
        <!-- Image Section -->
        <div style="flex: 0 0 50%; display: flex; justify-content: center; align-items: center;">
            <img src="projects/pics/multi_stage_yield.jpg" alt="Sensor Comparison" style="height: auto; width: 90%; max-height: 100%;">
        </div>

        <!-- Buttons and Text Section -->
        <div style="flex: 1; display: flex; flex-direction: column; padding: 20px;">
            <!-- Buttons Section -->
            <div style="flex: 0 0 20%; display: flex; justify-content: center; gap: 10px; align-items: center;">
                <!--<a href="https://osf.io/preprints/osf/sp2u9_v1" target="_blank" style="text-decoration: none;">
                    <button style="padding: 10px 20px; background-color: #5887FF; color: white; border: none; border-radius: 5px; cursor: pointer;">Paper</button>
                </a>-->
                <a href="slides/multiseason_yield_AGU_2024.pdf" target="_blank" style="text-decoration: none;">
                    <button style="padding: 10px 20px; background-color: #5887FF; color: white; border: none; border-radius: 5px; cursor: pointer;">Slides</button>
                </a>
                <a href="posters/yield_multi_poster.pdf" target="_blank" style="text-decoration: none;">
                    <button style="padding: 10px 20px; background-color: #5887FF; color: white; border: none; border-radius: 5px; cursor: pointer;">Poster</button>
                </a>
                <!--<a href="https://data.mendeley.com/datasets/v9b7rwrwx9/1" target="_blank" style="text-decoration: none;">
                    <button style="padding: 10px 20px; background-color: #5887FF; color: white; border: none; border-radius: 5px; cursor: pointer;">Data</button>
                </a>-->
                <a href="https://github.com/saif8091/yield-multi" target="_blank" style="text-decoration: none;">
                    <button style="padding: 10px 20px; background-color: #5887FF; color: white; border: none; border-radius: 5px; cursor: pointer;">Code</button>
                </a>
            </div>

            <!-- Text Section -->
            <div style="flex: 1; overflow-y: auto;">
                <h3 style="text-align: justify; margin: 0; font-size: 1.2vw; line-height: 1.6;">
                    Unmanned Aerial Systems (UAS) have demonstrated substantial potential for enhancing agricultural monitoring, yet their application to root crops remains underexplored. Key challenges include developing models that are robust across growth stages and seasons, and evaluating the comparative utility of diverse imaging modalities. This study addresses these gaps by introducing a unified Gaussian Process Regression (GPR) model for predicting end-of-season table beet (Beta vulgaris) root yield using UAS-derived spectral and structural features, along with meteorological data. The model is designed to be resilient to variations in both flight scheduling and harvest timing.

                    Field trials were conducted at Cornell AgriTech, Geneva, NY, during the 2021 and 2022 growing seasons. Data acquisition included five-band multispectral imagery (475, 560, 668, 717, and 840 nm), hyperspectral imagery spanning 400–1000 nm, and light detection and ranging (LiDAR), captured at multiple phenological stages. The unified model yielded an R²<sub>test</sub> of 0.81 and MAPE<sub>test</sub> of 15.7% using only multispectral data, while the fusion of hyperspectral and LiDAR data achieved an R²<sub>test</sub> of 0.79 and MAPE<sub>test</sub> of 17.4%. Model interpretation via SHAP (SHapley Additive exPlanations) analysis identified canopy volume as a primary contributor to yield prediction performance.
                    
                    This work demonstrates that scalable, sensor-agnostic machine learning models can deliver accurate, generalizable yield estimates for subterranean crops. All datasets and code used in this study will be made publicly available. The accompanying presentation slides correspond to our AGU24 talk, which focused specifically on the performance of the multispectral-based modeling approach.

                    </h3>
            </div>
        </div>
    </main>
</body>
</html>